{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPA7+FA+hlqK3xfqDuxj6hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsshan5388/assignmentchirag/blob/main/lstmdataset_rsudarshan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6UBnWq9lc0eA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w0q6KoddwMo",
        "outputId": "9d5a5886-17a3-4098-b041-dd3f3096908d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/lstm/LSTMDATA.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "zNfcuI2ueKJO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "text = text.lower()"
      ],
      "metadata": {
        "id": "q0irFlRsefhj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for line in text.split(\"\\n\"):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(2, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_len = max(len(x) for x in input_sequences)\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
        "\n",
        "# Split features and labels\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
      ],
      "metadata": {
        "id": "kn6U7zPYeriT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lstm_model():\n",
        "    model = Sequential([\n",
        "        Embedding(total_words, 100, input_length=max_seq_len - 1),\n",
        "        LSTM(150),\n",
        "        Dense(total_words, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "qmoGZ4Vye1Wm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = create_lstm_model()\n",
        "lstm_model.fit(X, y, epochs=20, verbose=1, callbacks=[EarlyStopping(patience=2)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8dfcsxve6dF",
        "outputId": "35aebe25-1f98-4f04-d575-06c5046f6a52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 39ms/step - accuracy: 0.0542 - loss: 6.4545\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 38ms/step - accuracy: 0.1209 - loss: 5.4360\n",
            "Epoch 3/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 39ms/step - accuracy: 0.1493 - loss: 4.9890\n",
            "Epoch 4/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 39ms/step - accuracy: 0.1715 - loss: 4.6551\n",
            "Epoch 5/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 39ms/step - accuracy: 0.1880 - loss: 4.3839\n",
            "Epoch 6/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 40ms/step - accuracy: 0.2082 - loss: 4.0955\n",
            "Epoch 7/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 39ms/step - accuracy: 0.2320 - loss: 3.8497\n",
            "Epoch 8/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 41ms/step - accuracy: 0.2599 - loss: 3.6176\n",
            "Epoch 9/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 39ms/step - accuracy: 0.2901 - loss: 3.4019\n",
            "Epoch 10/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 40ms/step - accuracy: 0.3221 - loss: 3.1991\n",
            "Epoch 11/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 40ms/step - accuracy: 0.3523 - loss: 3.0149\n",
            "Epoch 12/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 40ms/step - accuracy: 0.3878 - loss: 2.8236\n",
            "Epoch 13/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 40ms/step - accuracy: 0.4167 - loss: 2.6541\n",
            "Epoch 14/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 40ms/step - accuracy: 0.4448 - loss: 2.5114\n",
            "Epoch 15/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 41ms/step - accuracy: 0.4732 - loss: 2.3731\n",
            "Epoch 16/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 40ms/step - accuracy: 0.4962 - loss: 2.2543\n",
            "Epoch 17/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 41ms/step - accuracy: 0.5243 - loss: 2.1172\n",
            "Epoch 18/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 41ms/step - accuracy: 0.5486 - loss: 2.0121\n",
            "Epoch 19/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 41ms/step - accuracy: 0.5699 - loss: 1.9051\n",
            "Epoch 20/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 42ms/step - accuracy: 0.5896 - loss: 1.8181\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a35a2bb9010>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gru_model():\n",
        "    model = Sequential([\n",
        "        Embedding(total_words, 100, input_length=max_seq_len - 1),\n",
        "        GRU(150),\n",
        "        Dense(total_words, activation='softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "gru_model = create_gru_model()\n",
        "gru_model.fit(X, y, epochs=20, verbose=1, callbacks=[EarlyStopping(patience=2)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8I1ktD_qh_v",
        "outputId": "07078f80-a2b8-4e58-96f7-b19ace01bd17"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 41ms/step - accuracy: 0.0645 - loss: 6.3482\n",
            "Epoch 2/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 43ms/step - accuracy: 0.1365 - loss: 5.2046\n",
            "Epoch 3/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 42ms/step - accuracy: 0.1649 - loss: 4.7608\n",
            "Epoch 4/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 43ms/step - accuracy: 0.1901 - loss: 4.3625\n",
            "Epoch 5/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 42ms/step - accuracy: 0.2190 - loss: 4.0122\n",
            "Epoch 6/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 42ms/step - accuracy: 0.2503 - loss: 3.6927\n",
            "Epoch 7/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 42ms/step - accuracy: 0.2923 - loss: 3.3795\n",
            "Epoch 8/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 43ms/step - accuracy: 0.3346 - loss: 3.1115\n",
            "Epoch 9/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 44ms/step - accuracy: 0.3769 - loss: 2.8622\n",
            "Epoch 10/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 47ms/step - accuracy: 0.4168 - loss: 2.6509\n",
            "Epoch 11/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 45ms/step - accuracy: 0.4494 - loss: 2.4635\n",
            "Epoch 12/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 42ms/step - accuracy: 0.4859 - loss: 2.2834\n",
            "Epoch 13/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 42ms/step - accuracy: 0.5138 - loss: 2.1461\n",
            "Epoch 14/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 41ms/step - accuracy: 0.5425 - loss: 2.0171\n",
            "Epoch 15/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 41ms/step - accuracy: 0.5635 - loss: 1.9055\n",
            "Epoch 16/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 42ms/step - accuracy: 0.5831 - loss: 1.7982\n",
            "Epoch 17/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 42ms/step - accuracy: 0.6037 - loss: 1.7136\n",
            "Epoch 18/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 42ms/step - accuracy: 0.6186 - loss: 1.6418\n",
            "Epoch 19/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 42ms/step - accuracy: 0.6409 - loss: 1.5440\n",
            "Epoch 20/20\n",
            "\u001b[1m3341/3341\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 42ms/step - accuracy: 0.6495 - loss: 1.4955\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a35a2a3a390>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, seed_text, n_words=1):\n",
        "    for _ in range(n_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_seq_len - 1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        output_word = tokenizer.index_word[np.argmax(predicted)]\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "X8A1wEnD70-O"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next_word(lstm_model, \"it is a truth\"))\n",
        "print(predict_next_word(gru_model, \"i hope you\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXWCsMzJ7oam",
        "outputId": "3567a8d2-2a66-405f-a1c4-f62a822a0f83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it is a truth universally\n",
            "i hope you will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predict_next_word(lstm_model, \"what does it \"))\n",
        "print(predict_next_word(gru_model, \"should i \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJSpPQ_G75EV",
        "outputId": "0930b00f-058f-4032-fad6-f3b69345ca14"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what does it  you\n",
            "should i  believe\n"
          ]
        }
      ]
    }
  ]
}